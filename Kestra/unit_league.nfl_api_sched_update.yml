id: nfl_api_sched_update
namespace: unit_league

tasks:
  - id: fetch_weeks
    type: io.kestra.plugin.jdbc.postgresql.Query
    sql: |         
      select distinct    
        date_part('YEAR',fs2.gamedate::date) as year
        ,fs2.season_type as seasontype
        ,fs2.gameweek as weekno
      from src.foot_schedule fs2 
      where fs2.gamedate < CURRENT_DATE
        and fs2.gamedate  > '2025-07-01'
        and fs2.status_period  = 0	-- Not complete yet	  
        and  fs2.league_id  = 2 -- NFL
      ;
    fetchType: STORE

  - id: ion_to_csv
    type: "io.kestra.plugin.serdes.csv.IonToCsv"
    from: "{{ outputs.fetch_weeks.uri }}"

  - id: if
    type: io.kestra.plugin.core.flow.If
    condition: "{{ outputs['fetch_weeks'].size > 0 }}"
    then:
    - id: python_nfl_api
      type: io.kestra.plugin.scripts.python.Script
      outputFiles:
        - "nfl_schedule.csv"
      beforeCommands:
        - pip install pandas
      inputFiles:
        input.csv: "{{outputs.ion_to_csv.uri}}"
      warningOnStdErr: false
      script: |         
          import requests
          import pandas as pd
          import time

          input_df = pd.read_csv('input.csv')
          input_df['year'] = input_df['year'].astype(int)
          input_df['seasontype'] = input_df['seasontype'].astype(int)
          input_df['weekno'] = input_df['weekno'].astype(int)

          # Function to check if an element is a list or dictionary
          def is_list_or_dict(x):
              return isinstance(x, (list, dict))

          def merge_data(data):
              game_df = pd.json_normalize(data)
              game_df = game_df.drop(['uid'], axis=1)

              team_df = pd.json_normalize(data,
                                      record_path=['competitions', 'competitors'],
                                      meta=['id'],
                                      meta_prefix='game.')
              team_df = team_df.drop(['id', 'uid'], axis=1)    
              
              columns_to_keep = game_df.map(is_list_or_dict).all(axis=0) == False

              # Filter the DataFrame to keep only the desired columns
              game_df = game_df.loc[:, columns_to_keep]
              
              df = pd.merge(game_df, team_df, how='outer', left_on=['id'], right_on=['game.id']).drop(['id'], axis=1)

              # --- Split linescores into Q1-Q5 ---
              # Initialize Q1-Q5 columns to 0 if linescores column is missing
              for i in range(1, 6):
                  df[f'Q{i}'] = 0
              if 'linescores' in df.columns:
                  for idx, row in df.iterrows():
                      scores = row['linescores'] if isinstance(row['linescores'], list) else []
                      for s in scores:
                          period = s.get('period', 0)
                          value = s.get('value', 0)
                          if 1 <= period <= 5:
                              df.at[idx, f'Q{period}'] = value

              final_columns = [ 'date','name','shortName','week.number','season.year','season.type','season.slug','status.period'
                              ,'homeAway','score','winner','linescores','team.displayName','team.abbreviation','team.shortDisplayName'
                              ,'team.id','game.id','seasontype','seasontypeLabel','week','weekLabel','Q1','Q2','Q3','Q4','Q5']
              df = df.reindex(columns=final_columns)

              return df

          def collapse_games(df):

              # Split into home/away
              home_df = df[df['homeAway'] == 'home'].copy()
              away_df = df[df['homeAway'] == 'away'].copy()

              home_df.drop(['homeAway', 'winner', 'linescores'], axis=1, inplace=True)
              away_df.drop(['homeAway', 'winner', 'linescores'], axis=1, inplace=True)

              # Rename home/away columns
              home_df = home_df.rename(columns={
                  'score': 'home_score',
                  'team.displayName': 'home_team',
                  'team.abbreviation': 'home_abbr',
                  'team.shortDisplayName': 'home_short',
                  'team.id': 'home_team_id'
              })
              away_df = away_df.rename(columns={
                  'score': 'away_score',
                  'team.displayName': 'away_team',
                  'team.abbreviation': 'away_abbr',
                  'team.shortDisplayName': 'away_short',
                  'team.id': 'away_team_id'
              })

              home_df = home_df.rename(columns={f'Q{i}': f'Q{i}_home' for i in range(1, 6)})
              away_df = away_df.rename(columns={f'Q{i}': f'Q{i}_away' for i in range(1, 6)})

              # Merge on game.id and other shared columns
              merge_keys = ['game.id', 'date', 'name', 'shortName', 'week.number', 
                            'season.year', 'season.type', 'season.slug', 'status.period', 
                            'seasontype', 'seasontypeLabel', 'week', 'weekLabel']
              return pd.merge(home_df, away_df, on=merge_keys)

              
          headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko)  Chrome/58.0.3029.110 Safari/537.3"}
          dfs = []

          input_df

          # Loop through each row in input_df
          for _, row in input_df.iterrows():
              year = row["year"]
              seasontype = row["seasontype"]
              weekNo = row["weekno"]

              # Build URL with year, seasontype, and week number
              url = f'https://cdn.espn.com/core/nfl/schedule?xhr=1&year={year}&seasontype={seasontype}&week={weekNo}'
              jsonData = requests.get(url, headers=headers).json()

              # Get schedule from JSON
              schedules = jsonData['content']['schedule']
              # print(f'Acquiring {year} seasontype={seasontype}, week={weekNo}')

              for k, v in schedules.items():
                  games = v['games']

                  df = merge_data(games)
                  df['seasontype'] = seasontype
                  df['week'] = weekNo
                  # df['year'] = year

                  dfs.append(df)

              time.sleep(0.5)


          results = pd.concat(dfs)
          games = collapse_games(results)

          games = games[~(games['name'] == 'TBD TBD at TBD TBD')]
          games.columns = games.columns.str.replace('.', '_', regex=False)
          games = games.rename(columns={'date': 'gamedate', 'week': 'gameweek'})
          games = games.drop_duplicates(subset='game_id', keep='first')

          games.to_csv("nfl_schedule.csv", index=False)

    - id: pg_copyin_stg
      type: io.kestra.plugin.jdbc.postgresql.CopyIn
      format: CSV
      from: "{{ outputs.python_nfl_api.outputFiles['nfl_schedule.csv'] }}"
      table: api.stg_nfl_schedule  # Lowercase in DB
      header: true
      delimiter: ","

    - id: pg_exec_sp_load
      type: io.kestra.plugin.jdbc.postgresql.Query
      sql: |
        CALL "api"."sp_src_foot_load_ball"();

pluginDefaults:
  - type: io.kestra.plugin.jdbc.postgresql
    values:
      url: jdbc:postgresql://{{secret('POSTGRES_HOST')}}/SPORT
      username: "{{secret('POSTGRES_USERNAME')}}"
      password: "{{secret('POSTGRES_PASSWORD')}}"

triggers:
  - id: schedule
    type: io.kestra.plugin.core.trigger.Schedule
    cron: "0 11 * * *"
    recoverMissedSchedules: NONE
